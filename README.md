# ğŸš€ RAG QA Generator

Proyecto para generar preguntas sobre documentos utilizando **RAGs** (Retrieval-Augmented Generation) con **DeepEval** y modelos locales.

## ğŸ“ DescripciÃ³n del Proyecto
 
Este proyecto facilita la generaciÃ³n automÃ¡tica de preguntas a partir de un conjunto de documentos, implementando un flujo de **RecuperaciÃ³n** (Retrieval) y **GeneraciÃ³n** (Generation) basado en modelos de lenguaje local. Se integra con DeepEval para evaluar la calidad de las preguntas generadas y permite la configuraciÃ³n de servidores locales de modelos (VLLM y Ollama).

![RAG QA Generator](/imagenes/3_general.png)

---

## ğŸ’» Requisitos Previos âœ…

- Python 3.11 ğŸ
- pip ğŸ“¦ 
- Git ğŸ§
- Hardware compatible con VLLM y Ollama ğŸ’»

---

## ğŸ› ï¸ InstalaciÃ³n ğŸ”§

### 1. Crear un entorno virtual ğŸ¯

```bash
python3 -m venv venv
source venv/bin/activate
```

### 2. Instalar dependencias base ğŸ“¥

```bash
pip install -r requirements.txt
```

### 3. Configurar Hugging Face ğŸ¤—

Primero, instala y configura el CLI de Hugging Face:

```bash
huggingface-cli login
```

**ConfiguraciÃ³n del token de Hugging Face:**
- Ve a https://huggingface.co/settings/tokens
- Crea un nuevo token con permisos de lectura ğŸ”‘
- Usa la configuraciÃ³n mostrada en la imagen ğŸ“¸
![Hugginface ConfiguraciÃ³n Token](imagenes/hugginface_token.png)
- Introduce el token cuando se te solicite âœï¸

### 4. Instalar y configurar DeepEval ğŸ§ 

```bash
pip install -U deepeval
deepeval login --confident-api-key [TU-API-KEY-DE-DEEPEVAL]
```

**Obtener API key de DeepEval:**
- RegÃ­strate en https://deepeval.com/ ğŸ“
- Sigue las instrucciones en: https://deepeval.com/docs/getting-started
- DeepEval es gratuito y con su api-key permite mantener todos los resultados de evaluaciÃ³n en la nube â˜ï¸

### 5. Aplicar modificaciones personalizadas ğŸ’¡

**Importante:** Reemplaza los archivos modificados de la carpeta `codigo_modificado/` en tu instalaciÃ³n local de la librerÃ­a de DeepEval. ğŸ”„

---

## âš™ï¸ ConfiguraciÃ³n de Servidores ğŸ–¥ï¸

### Servidor VLLM (Terminal 1) ğŸ›ï¸

```bash
vllm serve meta-llama/Meta-Llama-3-8B-Instruct \
    --port 8000 \
    --dtype float16 \
    --max-model-len 4096 \
    --tool-call-parser llama3_json
```

### Servidor Ollama (Terminal 2) ğŸ–¥ï¸

**InstalaciÃ³n de Ollama (Ubuntu/Debian):**

```bash
# Actualizar sistema
sudo apt update 
sudo apt install curl -y

# Herramientas para detecciÃ³n de hardware
sudo apt install pciutils lshw -y 

# Instalar Ollama
curl -fsSL https://ollama.com/install.sh | sh
```

**Ejecutar servidor Ollama:**

```bash
ollama serve
```

### Descargar modelos necesarios (Terminal 3) ğŸ“¦

```bash
# Modelo de razonamiento
ollama pull deepseek-r1:1.5b

# Modelo de embeddings en espaÃ±ol
ollama pull jina/jina-embeddings-v2-base-es
```

### ğŸ”§ Configurar DeepEval con modelos locales ğŸ› ï¸

```bash
# Configurar modelo principal
deepeval set-local-model \
  --model-name="meta-llama/Meta-Llama-3-8B-Instruct" \
  --base-url="http://localhost:8000/v1/" \
  --api-key="not-needed"

# Configurar embeddings
deepeval set-ollama-embeddings deepseek-r1:1.5b \
  --base-url="http://localhost:11434"
```

---

## ğŸš€ Uso â–¶ï¸

### ConfiguraciÃ³n inicial ğŸ”§

Edita `main.py` para especificar:
- Ruta de documentos de entrada ğŸ“‚
- Ruta de salida (por defecto: `output/`) ğŸ“¤

### Ejecutar el generador â–¶ï¸

```bash
python3 main.py
```

### Opciones de ejecuciÃ³n âš™ï¸

Puedes saltar pasos especÃ­ficos usando flags:

```bash
python3 main.py --skip_step1 --skip_step2 --skip_step3
```

- `--skip_step1`: Saltar generaciÃ³n â­ï¸
- `--skip_step2`: Saltar reformulaciÃ³n â­ï¸
- `--skip_step3`: Saltar mÃ³dulo RAG â­ï¸

### Resultados ğŸ“Š

- Los resultados incrementales se guardan en la carpeta `output/` ğŸ“
- El dataset final incluye preguntas generadas y sus respuestas correspondientes ğŸ“š

---

## ğŸ“ˆ EvaluaciÃ³n ğŸ“

![MÃ³dulo de evaluaciÃ³n](imagenes/4_evaluacion.png)

Una vez generado el dataset, puedes evaluarlo ejecutando:

```bash
python3 mod_4_evaluar.py
```
Este script realizarÃ¡ la evaluaciÃ³n automÃ¡tica del dataset final. 